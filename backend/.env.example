# ======================================================================
# WebSocket Collaborative Editing - Backend Environment Configuration
# ======================================================================
# Copy this file to .env and update with your actual values
# IMPORTANT: Never commit the .env file to version control
# ======================================================================

# ----------------------------------------------------------------------
# JWT Authentication
# ----------------------------------------------------------------------
# Secret key for signing JWT tokens (CHANGE IN PRODUCTION!)
# Generate a secure key using: openssl rand -hex 32
# Minimum 256 bits recommended for HS256 algorithm
JWT_SECRET_KEY=your-secret-key-change-in-production-256-bit

# ----------------------------------------------------------------------
# CORS Configuration
# ----------------------------------------------------------------------
# Comma-separated list of allowed origins (NO SPACES)
# Include all frontend URLs that will connect to this backend
# Examples:
#   Development: http://localhost:3000,http://localhost:3003
#   Production:  https://yourdomain.com,https://app.yourdomain.com
CORS_ORIGINS=http://localhost:3000,http://localhost:3003

# ----------------------------------------------------------------------
# WebSocket Configuration
# ----------------------------------------------------------------------
# Size of chunks for splitting large file content (in bytes)
# Default: 102400 bytes (100KB)
# MUST match the CHUNK_SIZE_BYTES constant in frontend/lib/config.ts
# Larger values = fewer chunks, more memory per message
# Smaller values = more chunks, lower memory per message
CHUNK_SIZE_BYTES=102400

# ----------------------------------------------------------------------
# Deployment Configuration
# ----------------------------------------------------------------------
# Application deployment mode
# Values: 'development' or 'production'
# Affects logging verbosity and error detail exposure
DEPLOYMENT_MODE=development

# Reverse proxy configuration
# Set to 'true' if backend is behind nginx, Caddy, or other reverse proxy
# Set to 'false' for direct connections
# Affects WebSocket URL construction and CORS handling
USE_REVERSE_PROXY=false

# ----------------------------------------------------------------------
# Optional: Additional Configuration
# ----------------------------------------------------------------------
# Uncomment and configure as needed:

# Backend server host (default: 0.0.0.0 for all interfaces)
# HOST=0.0.0.0

# Backend server port (default: 8000)
# PORT=8000

# Maximum file size for WebSocket synchronization (in MB)
# MAX_FILE_SIZE_MB=10

# WebSocket ping interval in seconds (for keep-alive)
# WS_PING_INTERVAL=30

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# ----------------------------------------------------------------------
# LangSmith Observability & Tracing
# ----------------------------------------------------------------------
# Enable/disable LangSmith tracing for all LangGraph agent executions
# When enabled, all agent runs, tool calls, and LLM interactions are traced
# Values: 'true' to enable, 'false' to disable
LANGSMITH_TRACING=true

# LangSmith API key for authentication
# Obtain from: https://smith.langchain.com (Settings > API Keys)
# SECURITY: Keep this secret - never commit to version control
LANGSMITH_API_KEY=ls_your_api_key_here

# LangSmith project name for organizing traces
# Creates a new project in your LangSmith dashboard if it doesn't exist
# Use different project names for dev/staging/production environments
LANGSMITH_PROJECT=module-2-2-research-agent

# LangSmith API endpoint (typically no need to change)
# Default: https://api.smith.langchain.com
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# ----------------------------------------------------------------------
# Evaluation Configuration (TandemAI - Hybrid Setup)
# ----------------------------------------------------------------------
# Model used for LLM-as-a-judge evaluation agents
# Hybrid approach: Use local Ollama for judges (cost-effective) while keeping Gemini for researcher
# Options: qwen3-vl:2b (100% success, slow), qwen3-vl:4b (81% success, faster), gemini-2.5-flash (API)
# Default: qwen3-vl:2b (most reliable local model)
# Cost Impact: Local Ollama = $0, Gemini = ~$0.40 per 448 judge evaluations
JUDGE_MODEL=qwen3-vl:2b
