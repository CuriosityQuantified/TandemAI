# LangSmith Quick Start Guide

**5-Minute Setup for Phase 1 & 2**

---

## What You Just Got

Your research agent backend now automatically sends execution traces to LangSmith with:
- Complete agent execution graphs
- Tool call details (Tavily searches, file operations)
- LLM interactions (Claude Haiku 4.5)
- User-scoped metadata and tags
- Session tracking for conversations

**Zero code changes needed** - LangGraph handles it automatically!

---

## Quick Verification

### 1. Check Environment Variables

```bash
cat /Users/nicholaspate/Documents/01_Active/Corp_Strat/open-source-CC/docs/learning-plan/lessons/.env | grep LANGSMITH
```

**Expected Output:**
```
LANGSMITH_TRACING=true
LANGSMITH_API_KEY=lsv2_pt_...
LANGSMITH_PROJECT=module-2-2-research-agent
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
```

### 2. Start the Backend

```bash
# Navigate to backend directory
cd /Users/nicholaspate/Documents/01_Active/Corp_Strat/open-source-CC/docs/learning-plan/lessons/module-2-2/module-2-2-frontend-enhanced/backend

# Activate virtual environment
source /Users/nicholaspate/Documents/01_Active/Corp_Strat/open-source-CC/docs/learning-plan/lessons/.venv/bin/activate

# Start server
python main.py
```

**Look for:** Server starts without errors at http://localhost:8000

### 3. Send a Test Request

**Option A: Simple Test (No Authentication)**
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Write a hello world file", "auto_approve": true}'
```

**Option B: Authenticated Test (With User Tracking)**
```bash
# Get token
TOKEN=$(curl -s -X POST http://localhost:8000/api/auth/token | jq -r .access_token)

# Send request
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"message": "Research latest AI trends", "auto_approve": true}'
```

### 4. Check LangSmith Dashboard

1. Open: https://smith.langchain.com
2. Navigate to: **module-2-2-research-agent** project
3. Look for your recent trace

**What You'll See:**
- Execution graph showing agent steps
- Tool calls (tavily_search, write_file, etc.)
- LLM prompts and responses
- Timing data
- Metadata (user_id, session_id if authenticated)
- Tags for filtering

---

## Understanding the Trace

### Trace Structure

```
Run (Top-level agent execution)
â”œâ”€â”€ Agent (LangGraph agent)
â”‚   â”œâ”€â”€ LLM Call (Claude Haiku 4.5)
â”‚   â”œâ”€â”€ Tool: tavily_search
â”‚   â”‚   â””â”€â”€ Search results
â”‚   â”œâ”€â”€ LLM Call (Processing results)
â”‚   â”œâ”€â”€ Tool: write_file
â”‚   â”‚   â””â”€â”€ File written
â”‚   â””â”€â”€ LLM Call (Final response)
â””â”€â”€ Complete
```

### Metadata Fields

**All Traces:**
- `environment`: "development" or "production"
- `version`: "2.5"
- `session_id`: Unique UUID for this conversation

**Authenticated Traces:**
- `user_id`: User UUID from JWT token
- `thread_id`: Session ID for conversation persistence

### Tags

**All Traces:**
- `development` or `production`
- `research-agent`

**Authenticated Traces:**
- `user:<user_id>`

---

## Common Use Cases

### 1. Debug a Failed Request

**Steps:**
1. Go to LangSmith dashboard
2. Filter by date/time of failure
3. Click on the failed trace (red icon)
4. Expand execution tree
5. Find the error in the trace details
6. See exact inputs that caused the error

### 2. Track User Activity

**Steps:**
1. Go to LangSmith dashboard
2. Filter by tag: `user:<user_id>`
3. See all traces for that user
4. Analyze usage patterns
5. Debug user-specific issues

### 3. Monitor Performance

**Steps:**
1. Go to LangSmith dashboard
2. Check "Latency" column
3. Sort by duration
4. Identify slow traces
5. Expand to see which step is slow

### 4. Analyze Tool Usage

**Steps:**
1. Filter traces by tool name (e.g., "tavily_search")
2. See all searches performed
3. Check search quality
4. Identify common search patterns

---

## Tips & Tricks

### Filtering Traces

**By User:**
```
tag:user:123e4567-e89b-12d3-a456-426614174000
```

**By Environment:**
```
tag:development
```

**By Date Range:**
Use the date picker in the dashboard

**By Trace ID:**
Search by the trace ID directly

### Debugging Approval Workflows

When `auto_approve=false`, approval requests will show up in the trace:
1. Look for "Waiting for approval" events
2. Check approval decision metadata
3. See which user approved/rejected

### Performance Monitoring

**Check these metrics:**
- Total trace duration
- LLM call latency
- Tool call latency
- Queue time (for approval workflows)

---

## Disable Tracing (If Needed)

**Option 1: Environment Variable**
```bash
# Edit .env file
LANGSMITH_TRACING=false
```

**Option 2: Remove from Config**
```bash
# Comment out in .env
# LANGSMITH_TRACING=true
```

**Restart backend** for changes to take effect.

---

## Next Steps

### Phase 3: Production Architecture
- Set up PostgreSQL for persistent state
- Enable multi-server deployments
- Add custom traces for WebSocket events

### Phase 4: Advanced Observability
- Trace file operations with `@traceable`
- Trace WebSocket broadcasts
- Add approval workflow tracing

### Phase 5: Quality Evaluation
- Create evaluation datasets from traces
- Set up automated quality checks
- Build feedback collection API

---

## Troubleshooting

### "No traces appearing"
âœ… Check `LANGSMITH_TRACING=true` in .env
âœ… Verify API key is correct
âœ… Ensure internet connectivity
âœ… Check backend logs for errors

### "User ID not showing"
âœ… Send Authorization header: `Authorization: Bearer <token>`
âœ… Verify token is valid (not expired)
âœ… Check backend logs for JWT errors

### "Import errors"
âœ… Activate correct virtual environment
âœ… Run `pip install -r backend/requirements.txt`
âœ… Verify langsmith>=0.4.39 is installed

### "Pydantic warnings"
âœ… These are non-critical - functionality works fine
âœ… Already fixed in observability/config.py

---

## Resources

**Documentation:**
- LangSmith: https://docs.smith.langchain.com
- LangGraph: https://langchain-ai.github.io/langgraph/
- DeepAgents: https://github.com/deepagents/deepagents

**Dashboard:**
- https://smith.langchain.com

**Support:**
- LangSmith Community: https://github.com/langchain-ai/langsmith-sdk
- Issues: File in the backend repository

---

**Implementation Complete!** ðŸŽ‰

Your backend now has enterprise-grade observability with zero performance impact.

For detailed documentation, see:
- `LANGSMITH_PHASE_1_2_SUMMARY.md` - Full implementation details
- `LANGSMITH_INTEGRATION_PLAN.md` - Complete 6-phase roadmap
