# EvaluationResult Aggregation Function - Usage Guide

## Quick Reference

**Location**: `/Users/nicholaspate/Documents/01_Active/TandemAI/main/evaluation/judge_agents.py` (lines 524-661)

**Function**: `aggregate_judgments_to_evaluation_result()`

---

## Basic Usage

```python
from evaluation.judge_agents import aggregate_judgments_to_evaluation_result

# Prepare judge decisions (from all 7 judges)
judge_decisions = {
    'planning_quality': {'score': 1.0, 'reasoning': '...'},
    'execution_completeness': {'score': 5, 'reasoning': '...'},
    'source_quality': {'score': 4, 'reasoning': '...'},
    'citation_accuracy': {'score': 1.0, 'reasoning': '...'},
    'answer_completeness': {'score': 5, 'reasoning': '...'},
    'factual_accuracy': {'score': 1.0, 'reasoning': '...'},
    'autonomy_score': {'score': 1.0, 'reasoning': '...'}
}

# Create EvaluationResult
result = aggregate_judgments_to_evaluation_result(
    query_id=1,
    query_text="What is quantum computing?",
    prompt_version="benchmark",
    judge_decisions=judge_decisions,
    researcher_response="Full response text...",  # optional
    researcher_plan={'steps': [...]}  # optional
)

# Access scores
print(result.planning_quality.score)  # 1.0
print(result.execution_completeness.score)  # 5
print(result.planning_quality.reasoning)  # Full reasoning text
```

---

## Required Parameters

### `query_id: int`
- Unique identifier for the query
- Must match TestQuery.id

### `query_text: str`
- The original research query
- Human-readable question

### `prompt_version: str`
- Version identifier: "benchmark", "challenger_1", etc.
- Used for comparison

### `judge_decisions: Dict[str, Dict[str, Any]]`
- **Required keys**: All 7 judges must be present
  - `planning_quality`
  - `execution_completeness`
  - `source_quality`
  - `citation_accuracy`
  - `answer_completeness`
  - `factual_accuracy`
  - `autonomy_score`

- **Each value must have**:
  - `score`: float (0.0 or 1.0) or int (1-5)
  - `reasoning`: str (detailed explanation)

---

## Optional Parameters

### `researcher_response: str = ""`
- Full response text from researcher agent
- Useful for analysis and review

### `researcher_plan: Dict[str, Any] | None = None`
- Research plan generated by agent
- Structure: `{'steps': [...], 'status': '...'}`

---

## Return Value

Returns `EvaluationResult` Pydantic model with:

```python
EvaluationResult(
    query_id=1,
    query_text="...",
    prompt_version="benchmark",

    # 7 evaluation dimensions (Pydantic models)
    planning_quality=BinaryScore(score=1.0, reasoning="..."),
    execution_completeness=ScaledScore(score=5, reasoning="..."),
    source_quality=ScaledScore(score=4, reasoning="..."),
    citation_accuracy=BinaryScore(score=1.0, reasoning="..."),
    answer_completeness=ScaledScore(score=5, reasoning="..."),
    factual_accuracy=BinaryScore(score=1.0, reasoning="..."),
    autonomy_score=BinaryScore(score=1.0, reasoning="..."),

    # Metadata
    evaluation_timestamp="2025-11-13T12:34:56",
    judge_version="1.0"
)
```

---

## Score Types

### BinaryScore (0.0 or 1.0)
Used for:
- `planning_quality`
- `citation_accuracy`
- `factual_accuracy`
- `autonomy_score`

```python
result.planning_quality.score  # 0.0 or 1.0
result.planning_quality.reasoning  # str
```

### ScaledScore (1-5)
Used for:
- `execution_completeness`
- `source_quality`
- `answer_completeness`

```python
result.execution_completeness.score  # 1, 2, 3, 4, or 5
result.execution_completeness.reasoning  # str
```

---

## Error Handling

### Missing Judges
```python
# If any of the 7 judges is missing
raise ValueError(
    f"Missing judge decisions for: ['planning_quality', ...]. "
    f"Required: ['planning_quality', 'execution_completeness', ...]"
)
```

### Missing Scores
```python
# If a judge is present but score is None/missing
raise ValueError(f"Judge 'planning_quality' missing score")
```

### Type Conversion
```python
# Binary scores automatically converted to float
score=float(judge_decisions['planning_quality']['score'])

# Scaled scores automatically converted to int
score=int(judge_decisions['execution_completeness']['score'])
```

---

## JSON Serialization

```python
# Convert to dict for JSON
result_dict = result.model_dump()

# Save to file
import json
with open('result.json', 'w') as f:
    json.dump(result_dict, f, indent=2)

# Load from file
with open('result.json', 'r') as f:
    data = json.load(f)
    result = EvaluationResult(**data)
```

---

## Integration with test_runner.py

The test runner automatically uses this function:

```python
runner = EvaluationRunner(results_dir="./results")

# Run evaluation batch
results = runner.run_evaluation_batch(
    prompt_version="benchmark",
    researcher_agent=my_agent
)

# Structured results are automatically created and saved
# File: ./results/evaluation_results_benchmark.json
```

---

## Common Patterns

### Extract All Planning Scores
```python
planning_scores = [r.planning_quality.score for r in results]
mean_planning = sum(planning_scores) / len(planning_scores)
```

### Filter High-Performing Queries
```python
high_performers = [
    r for r in results
    if r.planning_quality.score == 1.0
    and r.execution_completeness.score >= 4
    and r.factual_accuracy.score == 1.0
]
```

### Compare Benchmark vs Challenger
```python
for benchmark, challenger in zip(benchmark_results, challenger_results):
    if challenger.planning_quality.score > benchmark.planning_quality.score:
        print(f"Query {benchmark.query_id}: Challenger improved")
```

---

## Validation Rules

1. **All 7 judges required** - no partial evaluations
2. **Scores must be valid**:
   - Binary: exactly 0.0 or 1.0
   - Scaled: exactly 1, 2, 3, 4, or 5
3. **Reasoning must be present** - empty string allowed but not None
4. **Query ID must be int** - positive integer
5. **Prompt version must be str** - non-empty string

---

## Best Practices

1. **Always check all 7 judges** before calling
2. **Use try-except** for error handling
3. **Log warnings** for incomplete evaluations
4. **Save results immediately** after creation
5. **Use model_dump()** for JSON serialization
6. **Don't modify** EvaluationResult after creation (immutable)

---

## Troubleshooting

### "Missing judge decisions for..."
- Check all 7 judges are in judge_decisions dict
- Verify spelling of judge names (use underscores not hyphens)

### "Judge 'X' missing score"
- Ensure each judge has 'score' key
- Score value must not be None

### "Pydantic validation error"
- Check binary scores are 0.0 or 1.0
- Check scaled scores are 1-5
- Verify reasoning is a string

### "JSON serialization failed"
- Use `model_dump()` not `dict()`
- Don't include Python objects (use primitives only)

---

## Examples

### Example 1: Perfect Score
```python
perfect_decisions = {
    'planning_quality': {'score': 1.0, 'reasoning': 'Excellent plan'},
    'execution_completeness': {'score': 5, 'reasoning': 'All steps done'},
    'source_quality': {'score': 5, 'reasoning': 'Authoritative sources'},
    'citation_accuracy': {'score': 1.0, 'reasoning': 'Perfect citations'},
    'answer_completeness': {'score': 5, 'reasoning': 'Comprehensive'},
    'factual_accuracy': {'score': 1.0, 'reasoning': 'All facts verified'},
    'autonomy_score': {'score': 1.0, 'reasoning': 'Fully autonomous'}
}
```

### Example 2: Poor Score
```python
poor_decisions = {
    'planning_quality': {'score': 0.0, 'reasoning': 'No plan created'},
    'execution_completeness': {'score': 2, 'reasoning': 'Minimal execution'},
    'source_quality': {'score': 2, 'reasoning': 'Unreliable sources'},
    'citation_accuracy': {'score': 0.0, 'reasoning': 'No citations'},
    'answer_completeness': {'score': 2, 'reasoning': 'Incomplete answer'},
    'factual_accuracy': {'score': 0.0, 'reasoning': 'Major errors'},
    'autonomy_score': {'score': 0.0, 'reasoning': 'Required prompting'}
}
```

### Example 3: Mixed Score
```python
mixed_decisions = {
    'planning_quality': {'score': 1.0, 'reasoning': 'Good plan'},
    'execution_completeness': {'score': 4, 'reasoning': 'Mostly complete'},
    'source_quality': {'score': 3, 'reasoning': 'Acceptable sources'},
    'citation_accuracy': {'score': 1.0, 'reasoning': 'Proper citations'},
    'answer_completeness': {'score': 4, 'reasoning': 'Good coverage'},
    'factual_accuracy': {'score': 1.0, 'reasoning': 'Accurate'},
    'autonomy_score': {'score': 1.0, 'reasoning': 'Autonomous'}
}
```

---

**For detailed implementation**: See CRITICAL_FIX_4_SUMMARY.md

**For rubric details**: See evaluation/rubrics.py

**For judge implementation**: See evaluation/judge_agents.py

**For test runner usage**: See evaluation/test_runner.py
