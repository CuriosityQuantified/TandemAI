# LangChain Native Model Integrations

**Research Date**: November 14, 2025
**Purpose**: Document LangChain v1.0+ built-in integrations for Google Gemini and Groq to avoid separate SDKs

---

## âœ… Google Gemini Integration

### Package Information
- **Package Name**: `langchain-google-genai`
- **Latest Version**: 3.0.3 (Released: November 12, 2025)
- **Python Requirements**: >=3.10.0, <4.0.0
- **License**: MIT

### Installation
```bash
pip install langchain-google-genai
```

### Basic Usage
```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Initialize model
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key="YOUR_GOOGLE_API_KEY"  # or set GOOGLE_API_KEY env var
)

# Use with LangGraph
from langgraph.graph import StateGraph
# model can be used directly in LangGraph nodes
```

### Supported Models (2025)
- `gemini-2.5-pro` - Latest flagship model
- `gemini-2.5-flash` - Fast, efficient model ($0.30/$2.50 per MTok)
- `gemini-2.0-flash` - Previous generation flash model
- `gemini-2.5-flash-preview-tts` - Audio generation model

### Key Features
- âœ… Chat completion
- âœ… Vision capabilities
- âœ… Embeddings
- âœ… Audio generation
- âœ… Structured output (JSON schema)
- âœ… Native streaming support
- âœ… Batch processing
- âœ… LangGraph compatible

### Documentation Links
- **API Reference**: https://reference.langchain.com/python/integrations/langchain_google_genai/
- **PyPI**: https://pypi.org/project/langchain-google-genai/
- **GitHub**: https://github.com/langchain-ai/langchain-google
- **Changelog**: https://github.com/langchain-ai/langchain-google/releases?q=%22genai%22

---

## âœ… Groq Integration

### Package Information
- **Package Name**: `langchain-groq`
- **Latest Version**: 1.0.1 (Released: November 13, 2025)
- **Python Requirements**: >=3.10.0, <4.0.0
- **License**: MIT

### Installation
```bash
pip install langchain-groq
```

### Basic Usage
```python
from langchain_groq import ChatGroq

# Initialize model
model = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.7,
    groq_api_key="YOUR_GROQ_API_KEY"  # or set GROQ_API_KEY env var
)

# Use with LangGraph
from langgraph.graph import StateGraph
# model can be used directly in LangGraph nodes
```

### Supported Models
- `llama-3.3-70b-versatile` - Latest Llama model
- `mixtral-8x7b-32768` - Mixture of Experts model
- `deepseek-r1-distill-llama-70b` - DeepSeek R1 distilled
- `kimi-k2-thinking` - Kimi K2-thinking model ($0.60/$2.50 per MTok)

### Key Features
- âœ… Chat completion
- âœ… Standard Runnable Interface
- âœ… Native streaming support
- âœ… Batch processing
- âœ… LangGraph compatible
- âœ… Fast inference via Groq LPU technology
- âœ… Methods: `with_types`, `with_retry`, `assign`, `bind`, `get_graph`

### Documentation Links
- **API Reference**: https://reference.langchain.com/python/integrations/langchain_groq/
- **PyPI**: https://pypi.org/project/langchain-groq/
- **GitHub**: https://github.com/langchain-ai/langchain/tree/master/libs/partners/groq
- **Official Docs**: https://console.groq.com/docs/langchain

---

## ðŸ”§ Implementation for TandemAI

### Current Status
Both integrations are **production-ready** and **actively maintained** with regular updates throughout 2024-2025.

### Integration Approach
Instead of using separate SDKs:
- âŒ `google-generativeai` (separate SDK)
- âŒ `groq` (separate SDK)

Use LangChain native integrations:
- âœ… `langchain-google-genai` (unified LangChain interface)
- âœ… `langchain-groq` (unified LangChain interface)

### Benefits
1. **Unified Interface**: Same API across all models
2. **LangGraph Compatible**: Direct integration with LangGraph workflows
3. **No SDK Conflicts**: Avoid version conflicts between separate SDKs
4. **Better Maintenance**: Official LangChain support and updates
5. **Streaming Support**: Built-in streaming for all models
6. **Type Safety**: Full Python type annotations

---

## ðŸ“Š Cost Comparison (from ESTIMATE_PHASE_6_COST.py)

| Model | Provider | Input $/MTok | Output $/MTok | Total Cost* |
|-------|----------|--------------|---------------|-------------|
| Claude 3.5 Haiku | Anthropic | $0.25 | $1.25 | $0.57 |
| Gemini 2.5 Flash | Google | $0.30 | $2.50 | $0.99 |
| Kimi K2-thinking | Groq | $0.60 | $2.50 | $1.36 |
| Claude 4.5 Haiku | Anthropic | $1.00 | $5.00 | $2.27 |

*For 32 queries Ã— 7 judges = 224 evaluations with prompt caching

---

## ðŸš€ Next Steps

1. âœ… Research completed - both integrations confirmed
2. ðŸ”„ Update VERIFY_MODEL_COSTS.py to use native integrations
3. â³ Install dependencies: `langchain-google-genai`, `langchain-groq`
4. â³ Test single-query verification with all 4 models
5. â³ Run full Phase 6 benchmark evaluation (224 judgments)

---

## ðŸ“ Code Examples

### Anthropic (Claude)
```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-haiku-20241022",
    temperature=0.7
)
```

### Google (Gemini)
```python
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7
)
```

### Groq (Multi-Model)
```python
from langchain_groq import ChatGroq

model = ChatGroq(
    model="kimi-k2-thinking",  # or llama-3.3-70b-versatile, mixtral-8x7b-32768
    temperature=0.7
)
```

### LangGraph Integration (All Models)
```python
from langgraph.graph import StateGraph, MessagesState
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq

# Any of these can be used in LangGraph nodes
def create_node(model):
    def node(state: MessagesState):
        response = model.invoke(state["messages"])
        return {"messages": [response]}
    return node

# Build graph with any model
workflow = StateGraph(MessagesState)
workflow.add_node("agent", create_node(model))
graph = workflow.compile()
```

---

**Status**: âœ… CONFIRMED - Both integrations are official, actively maintained, and production-ready
